{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c6bd7f",
   "metadata": {},
   "source": [
    "# Notebook visualizing GAN inversion and learned latent spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import continual_benchmark.dataloaders as dataloaders\n",
    "import continual_benchmark.dataloaders.base\n",
    "from continual_benchmark.dataloaders.datasetGen import data_split\n",
    "from gan_experiments.models_definition import Generator, Translator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287827c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES_TO_OPTIMIZE = 1000 # number of images to optimize noise for\n",
    "DATAROOT = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "NUM_BATCHES = 10\n",
    "DATASET = 'FashionMNIST'\n",
    "MODELS_DIR = os.path.join('..', 'results', DATASET, 'Dirichlet_alpha_1_10')\n",
    "DEVICE = 'cuda'\n",
    "N_IMGS_TO_VISUALIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c13e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_noise(images, generator, n_iterations, task_id, lr):\n",
    "    generator.eval()\n",
    "\n",
    "    images = images.to(generator.device)\n",
    "    task_ids = (torch.zeros([len(images)]) + task_id).to(generator.device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    noise = torch.randn(len(images), generator.latent_dim).to(generator.device)\n",
    "    noise.requires_grad = True\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    optimizer = torch.optim.Adam([noise], lr=lr)\n",
    "    for i in range(n_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        generations = generator(noise, task_ids)\n",
    "        loss = criterion(generations, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {i}/{n_iterations}] [Loss: {loss.item():.3f}]\"\n",
    "            )\n",
    "\n",
    "    return noise, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830423cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_image_grid(images):\n",
    "    fig = plt.figure(figsize=(2*len(images), 2))\n",
    "    grid = ImageGrid(fig, 111,\n",
    "                     nrows_ncols=(1, len(images)),\n",
    "                     axes_pad=0.2,\n",
    "                     )\n",
    "\n",
    "    for ax, im in zip(grid, images):\n",
    "        if DATASET.lower() in [\"cifar10\", \"celeba\"]:\n",
    "            im = im / 2 + 0.5\n",
    "        \n",
    "        im = np.swapaxes(im, 0, 2)\n",
    "        im = np.swapaxes(im, 0, 1)\n",
    "        if im.shape[2] == 1:\n",
    "            ax.imshow(im, cmap=\"gray\" if DATASET.lower() in ['mnist', 'fashionmnist', 'omniglot', 'doublemnist'] else None)\n",
    "        else:\n",
    "            ax.imshow(im)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataloaders.base.__dict__[DATASET](\n",
    "        DATAROOT, False, False\n",
    "    )\n",
    "\n",
    "train_dataset_splits, val_dataset_splits, task_output_space = data_split(\n",
    "    dataset=train_dataset,\n",
    "    dataset_name=DATASET,\n",
    "    num_batches=NUM_BATCHES,\n",
    "    num_classes=train_dataset.number_classes,\n",
    "    random_split=False,\n",
    "    random_mini_shuffle=False,\n",
    "    limit_data=None,\n",
    "    dirichlet_split_alpha=1,\n",
    "    reverse=False,\n",
    "    limit_classes=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d10a8",
   "metadata": {},
   "source": [
    "## GAN inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45f047",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lrs = [0.1]\n",
    "n_iterations = 1000\n",
    "results_dict = dict()\n",
    "\n",
    "for task_id in range(3):\n",
    "    display(Markdown(f\"# <span style='color:blue'>Task nr {task_id}</span>\"))\n",
    "    \n",
    "    results_dict[task_id] = dict()\n",
    "    # select random imgs from current task to optimize\n",
    "    random_img_idxs = np.random.choice(len(train_dataset_splits[task_id].dataset), size=N_IMAGES_TO_OPTIMIZE, replace=False)\n",
    "    random_imgs = torch.stack([train_dataset_splits[task_id].dataset[i][0] for i in random_img_idxs]).to(DEVICE)\n",
    "    random_classes = [train_dataset_splits[task_id].dataset[i][1] for i in random_img_idxs]\n",
    "    # select random indexes to visualize\n",
    "    random_img_vis_idxs = np.random.choice(N_IMAGES_TO_OPTIMIZE, size=N_IMGS_TO_VISUALIZE, replace=False)\n",
    "            \n",
    "    task_ids = (torch.zeros([N_IMAGES_TO_OPTIMIZE]) + task_id).to(DEVICE)\n",
    "    \n",
    "    generator = torch.load(os.path.join(MODELS_DIR, f'model{task_id}_curr_local_generator'), map_location=DEVICE)\n",
    "    \n",
    "    for lr in lrs:\n",
    "        display(Markdown(f\"### Learning rate of noise optimization = {lr}\"))\n",
    "        optimized_noise, losses = optimize_noise(\n",
    "            random_imgs,\n",
    "            generator,\n",
    "            n_iterations,\n",
    "            task_id,\n",
    "            lr,\n",
    "        )\n",
    "        results_dict[task_id][lr] = (optimized_noise, losses, random_classes)\n",
    "        \n",
    "        display(Markdown(f\"### Original images:\"))\n",
    "        vis_image_grid(random_imgs.cpu().numpy()[random_img_vis_idxs])\n",
    "        generations_from_optimized_noise = generator(optimized_noise, task_ids)\n",
    "        display(Markdown(f\"### Generations from optimized noise:\"))\n",
    "        vis_image_grid(generations_from_optimized_noise[random_img_vis_idxs].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for task_id in range(NUM_BATCHES):\n",
    "    for lr, (noise, loss, _) in results_dict[task_id].items():\n",
    "        plt.plot(loss, label=f\"task nr {task_id}\", linewidth=2)\n",
    "        \n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('MSE loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title(f'GAN inversion for each task - {DATASET}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8afd2e",
   "metadata": {},
   "source": [
    "## Visualization of optimized noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from matplotlib.offsetbox import (OffsetImage, AnnotationBbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_noises = torch.cat([results_dict[task_id][0.1][0] for task_id in range(NUM_BATCHES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_noises.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = reducer.fit_transform(concat_noises.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91227fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92464dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "for task_id in range(NUM_BATCHES):\n",
    "    xs = emb[task_id*N_IMAGES_TO_OPTIMIZE:(task_id+1)*N_IMAGES_TO_OPTIMIZE, 0]\n",
    "    ys = emb[task_id*N_IMAGES_TO_OPTIMIZE:(task_id+1)*N_IMAGES_TO_OPTIMIZE, 1]\n",
    "\n",
    "    ax.scatter(xs, ys, label=f\"task nr {task_id}\", s=16, alpha=0.5)\n",
    "    \n",
    "    generator = torch.load(os.path.join(MODELS_DIR, f'model{task_id}_curr_local_generator'), map_location=DEVICE)\n",
    "    task_ids = (torch.zeros([1]) + task_id).to(DEVICE)\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        if i % 100 == 0:\n",
    "            # generate\n",
    "            n = concat_noises[i].unsqueeze(0)\n",
    "            generation = generator(n.to(DEVICE), task_ids)[0]\n",
    "            generation = generation.cpu().detach().numpy()\n",
    "            generation = np.swapaxes(generation, 0, 2)\n",
    "            generation = np.swapaxes(generation, 0, 1)\n",
    "            imagebox = OffsetImage(generation, zoom = 0.7, cmap=\"gray\")\n",
    "            ab = AnnotationBbox(imagebox, (x, y), frameon = False)\n",
    "            ax.add_artist(ab)\n",
    "    \n",
    "plt.title(f'Optimized noises visualization for each task - {DATASET}')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d65cf",
   "metadata": {},
   "source": [
    "## Visualization of optimized noises per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b138b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1)\n",
    "fig.set_figheight(30)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "for task_id in range(3):\n",
    "    for lr, (noise, loss, classes) in results_dict[task_id].items():\n",
    "        reducer = umap.UMAP()\n",
    "        emb = reducer.fit_transform(noise.cpu().detach().numpy())\n",
    "        generator = torch.load(os.path.join(MODELS_DIR, f'model{task_id}_curr_local_generator'), map_location=DEVICE)\n",
    "        task_ids = (torch.zeros([N_IMAGES_TO_OPTIMIZE]) + task_id).to(DEVICE)\n",
    "        generations = generator(noise, task_ids)\n",
    "        for i in np.unique(classes):\n",
    "            xs = emb[np.array(classes)==i][:,0]\n",
    "            ys = emb[np.array(classes)==i][:,1]\n",
    "            gen = generations[np.array(classes)==i]\n",
    "            \n",
    "            ax[task_id].scatter(xs, ys, label=f\"class: {train_dataset.dataset.classes[i]}\", s=16, alpha=0.5)\n",
    "            ax[task_id].set_title(f\"Optimized noises for task nr {task_id} - {DATASET}\")\n",
    "            ax[task_id].axis('off')\n",
    "            ax[task_id].legend()\n",
    "            \n",
    "\n",
    "            for j in range(len(gen)):\n",
    "                if j % 50 == 0:\n",
    "                    generation = gen[j].cpu().detach().numpy()\n",
    "                    generation = np.swapaxes(generation, 0, 2)\n",
    "                    generation = np.swapaxes(generation, 0, 1)\n",
    "                    imagebox = OffsetImage(generation, zoom = 0.7, cmap=\"gray\")\n",
    "                    ab = AnnotationBbox(imagebox, (xs[j], ys[j]), frameon = False)\n",
    "                    ax[task_id].add_artist(ab)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71eed0",
   "metadata": {},
   "source": [
    "## Visualization of optimized noises in local translator's latent space per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec766d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1)\n",
    "fig.set_figheight(30)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "for task_id in range(3):\n",
    "    for lr, (noise, loss, classes) in results_dict[task_id].items():\n",
    "        reducer = umap.UMAP()\n",
    "        generator = torch.load(os.path.join(MODELS_DIR, f'model{task_id}_curr_local_generator'), map_location=DEVICE)\n",
    "        task_ids = (torch.zeros([N_IMAGES_TO_OPTIMIZE]) + task_id).to(DEVICE)\n",
    "        generations, translator_emb = generator(noise, task_ids, return_emb=True)\n",
    "\n",
    "        emb = reducer.fit_transform(translator_emb.cpu().detach().numpy())\n",
    "        for i in np.unique(classes):\n",
    "            xs = emb[np.array(classes)==i][:,0]\n",
    "            ys = emb[np.array(classes)==i][:,1]\n",
    "            gen = generations[np.array(classes)==i]\n",
    "            \n",
    "            ax[task_id].scatter(xs, ys, label=f\"class: {train_dataset.dataset.classes[i]}\", s=16, alpha=0.5)\n",
    "            ax[task_id].set_title(f\"Optimized noises in local translator\\'s latent space of task nr {task_id} - {DATASET}\")\n",
    "            ax[task_id].axis('off')\n",
    "            ax[task_id].legend()\n",
    "            \n",
    "\n",
    "            for j in range(len(gen)):\n",
    "                if j % 50 == 0:\n",
    "                    generation = gen[j].cpu().detach().numpy()\n",
    "                    generation = np.swapaxes(generation, 0, 2)\n",
    "                    generation = np.swapaxes(generation, 0, 1)\n",
    "                    imagebox = OffsetImage(generation, zoom = 0.7, cmap=\"gray\")\n",
    "                    ab = AnnotationBbox(imagebox, (xs[j], ys[j]), frameon = False)\n",
    "                    ax[task_id].add_artist(ab)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d697e7",
   "metadata": {},
   "source": [
    "## Visualization of global latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_generator = torch.load(os.path.join(MODELS_DIR, f'model9_curr_global_generator'), map_location=DEVICE)\n",
    "all_noises = []\n",
    "all_emb = []\n",
    "all_task_ids = []\n",
    "\n",
    "for task_id in range(3):\n",
    "    rand_noise = torch.randn([500, 100]).to(DEVICE)\n",
    "    task_ids = (torch.zeros([500]) + task_id).to(DEVICE)\n",
    "    generations, emb = global_generator(rand_noise, task_ids, return_emb=True)\n",
    "    all_noises.append(rand_noise)\n",
    "    all_emb.append(emb)\n",
    "    all_task_ids.append(task_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b07dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noises = torch.cat([x for x in all_noises])\n",
    "all_emb = torch.cat([x for x in all_emb])\n",
    "all_task_ids = torch.cat([x for x in all_task_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "emb = reducer.fit_transform(all_emb.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c826b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300763e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for task_id in range(3):\n",
    "    xs = emb[task_id*500:(task_id+1)*500,0]\n",
    "    ys = emb[task_id*500:(task_id+1)*500,1]\n",
    "    ax.scatter(xs, ys, label=f\"task nr {task_id}\", s=16, alpha=0.5)\n",
    "    \n",
    "    for j, (x, y) in enumerate(zip(xs, ys)):\n",
    "        if j % 100 == 0:\n",
    "            # generate\n",
    "            n = all_noises[task_id*500 + j].unsqueeze(0)\n",
    "            generation = global_generator(n.to(DEVICE), (torch.zeros([1]) + task_id).to(DEVICE))[0]\n",
    "            generation = generation.cpu().detach().numpy()\n",
    "            generation = np.swapaxes(generation, 0, 2)\n",
    "            generation = np.swapaxes(generation, 0, 1)\n",
    "            imagebox = OffsetImage(generation, zoom = 0.7, cmap=\"gray\")\n",
    "            ab = AnnotationBbox(imagebox, (x, y), frameon = False)\n",
    "            ax.add_artist(ab)\n",
    "\n",
    "plt.title(f\"Global latent space - {DATASET}\")    \n",
    "plt.axis('off')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124b1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
